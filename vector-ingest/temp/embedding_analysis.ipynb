{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Embedding Statistical Analysis\n",
        "\n",
        "This notebook provides comprehensive statistical analysis of embeddings generated from processed document chunks.\n",
        "\n",
        "## Data Source\n",
        "- **File**: `../output/processed_chunks.json`\n",
        "- **Content**: Document chunks with embeddings, metadata, and content analysis\n",
        "\n",
        "## Analysis Overview\n",
        "1. **Chunk Size Distribution** - Histogram of chunk sizes (word count)\n",
        "2. **Embedding Dimensionality Analysis** - Vector dimensions and properties\n",
        "3. **Content Type Analysis** - Distribution of chunk types and sections\n",
        "4. **Embedding Quality Metrics** - Statistical properties of embedding vectors\n",
        "5. **Correlation Analysis** - Relationships between chunk properties and embeddings\n",
        "6. **Clustering Analysis** - Natural groupings in the embedding space\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìö Libraries imported successfully!\n",
            "üé® Plotting style configured\n"
          ]
        },
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
            "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
            "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "# Import Required Libraries\n",
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from pathlib import Path\n",
        "from collections import Counter\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set up plotting style\n",
        "plt.style.use('default')\n",
        "sns.set_palette(\"husl\")\n",
        "plt.rcParams['figure.figsize'] = (12, 8)\n",
        "plt.rcParams['font.size'] = 10\n",
        "\n",
        "print(\"üìö Libraries imported successfully!\")\n",
        "print(\"üé® Plotting style configured\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load and Explore Data\n",
        "def load_processed_chunks(file_path):\n",
        "    \"\"\"Load processed chunks from JSON file.\"\"\"\n",
        "    try:\n",
        "        with open(file_path, 'r', encoding='utf-8') as f:\n",
        "            data = json.load(f)\n",
        "        print(f\"‚úÖ Successfully loaded {data['total_chunks']} chunks\")\n",
        "        return data\n",
        "    except FileNotFoundError:\n",
        "        print(f\"‚ùå File not found: {file_path}\")\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error loading file: {e}\")\n",
        "        return None\n",
        "\n",
        "# Load the data\n",
        "data_path = Path(\"../output/processed_chunks.json\")\n",
        "data = load_processed_chunks(data_path)\n",
        "\n",
        "if data:\n",
        "    chunks = data['chunks']\n",
        "    total_chunks = data['total_chunks']\n",
        "    \n",
        "    print(f\"\\nüìä Dataset Overview:\")\n",
        "    print(f\"   Total chunks: {total_chunks:,}\")\n",
        "    print(f\"   Data structure: {type(chunks[0]) if chunks else 'No chunks'}\")\n",
        "    \n",
        "    # Show sample chunk structure\n",
        "    if chunks:\n",
        "        sample_chunk = chunks[0]\n",
        "        print(f\"\\nüîç Sample Chunk Structure:\")\n",
        "        for key, value in sample_chunk.items():\n",
        "            if key == 'embedding':\n",
        "                print(f\"   {key}: vector of length {len(value)}\")\n",
        "            elif key == 'content':\n",
        "                print(f\"   {key}: {len(str(value))} characters\")\n",
        "            else:\n",
        "                print(f\"   {key}: {value}\")\n",
        "else:\n",
        "    print(\"‚ùå Cannot proceed without data\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'data' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# 1. Chunk Size Distribution Analysis\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mdata\u001b[49m \u001b[38;5;129;01mand\u001b[39;00m chunks:\n\u001b[32m      4\u001b[39m     \u001b[38;5;66;03m# Extract word counts\u001b[39;00m\n\u001b[32m      5\u001b[39m     word_counts = [chunk.get(\u001b[33m'\u001b[39m\u001b[33mword_count\u001b[39m\u001b[33m'\u001b[39m, \u001b[32m0\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m chunks]\n\u001b[32m      7\u001b[39m     \u001b[38;5;66;03m# Create comprehensive statistics\u001b[39;00m\n",
            "\u001b[31mNameError\u001b[39m: name 'data' is not defined"
          ]
        }
      ],
      "source": [
        "# 1. Chunk Size Distribution Analysis\n",
        "\n",
        "if data and chunks:\n",
        "    # Extract word counts\n",
        "    word_counts = [chunk.get('word_count', 0) for chunk in chunks]\n",
        "    \n",
        "    # Create comprehensive statistics\n",
        "    word_count_stats = {\n",
        "        'count': len(word_counts),\n",
        "        'mean': np.mean(word_counts),\n",
        "        'median': np.median(word_counts),\n",
        "        'std': np.std(word_counts),\n",
        "        'min': np.min(word_counts),\n",
        "        'max': np.max(word_counts),\n",
        "        'q25': np.percentile(word_counts, 25),\n",
        "        'q75': np.percentile(word_counts, 75)\n",
        "    }\n",
        "    \n",
        "    # Create figure with subplots\n",
        "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))\n",
        "    fig.suptitle('üìä Chunk Size Distribution Analysis', fontsize=16, fontweight='bold')\n",
        "    \n",
        "    # 1. Main histogram\n",
        "    ax1.hist(word_counts, bins=50, alpha=0.7, color='skyblue', edgecolor='black')\n",
        "    ax1.axvline(word_count_stats['mean'], color='red', linestyle='--', linewidth=2, label=f\"Mean: {word_count_stats['mean']:.1f}\")\n",
        "    ax1.axvline(word_count_stats['median'], color='green', linestyle='--', linewidth=2, label=f\"Median: {word_count_stats['median']:.1f}\")\n",
        "    ax1.set_xlabel('Word Count')\n",
        "    ax1.set_ylabel('Frequency')\n",
        "    ax1.set_title('Distribution of Chunk Sizes (Word Count)')\n",
        "    ax1.legend()\n",
        "    ax1.grid(True, alpha=0.3)\n",
        "    \n",
        "    # 2. Box plot\n",
        "    ax2.boxplot(word_counts, vert=True, patch_artist=True, \n",
        "                boxprops=dict(facecolor='lightblue', alpha=0.7))\n",
        "    ax2.set_ylabel('Word Count')\n",
        "    ax2.set_title('Box Plot of Chunk Sizes')\n",
        "    ax2.grid(True, alpha=0.3)\n",
        "    \n",
        "    # 3. Cumulative distribution\n",
        "    sorted_counts = np.sort(word_counts)\n",
        "    cumulative = np.arange(1, len(sorted_counts) + 1) / len(sorted_counts)\n",
        "    ax3.plot(sorted_counts, cumulative, linewidth=2, color='purple')\n",
        "    ax3.set_xlabel('Word Count')\n",
        "    ax3.set_ylabel('Cumulative Probability')\n",
        "    ax3.set_title('Cumulative Distribution of Chunk Sizes')\n",
        "    ax3.grid(True, alpha=0.3)\n",
        "    \n",
        "    # 4. Statistics table\n",
        "    ax4.axis('off')\n",
        "    stats_text = f\"\"\"\n",
        "    üìà Chunk Size Statistics\n",
        "    \n",
        "    Count:      {word_count_stats['count']:,}\n",
        "    Mean:       {word_count_stats['mean']:.2f}\n",
        "    Median:     {word_count_stats['median']:.2f}\n",
        "    Std Dev:    {word_count_stats['std']:.2f}\n",
        "    \n",
        "    Min:        {word_count_stats['min']:,}\n",
        "    Max:        {word_count_stats['max']:,}\n",
        "    Q25:        {word_count_stats['q25']:.2f}\n",
        "    Q75:        {word_count_stats['q75']:.2f}\n",
        "    \n",
        "    IQR:        {word_count_stats['q75'] - word_count_stats['q25']:.2f}\n",
        "    \"\"\"\n",
        "    ax4.text(0.1, 0.9, stats_text, transform=ax4.transAxes, fontsize=12,\n",
        "             verticalalignment='top', fontfamily='monospace',\n",
        "             bbox=dict(boxstyle='round', facecolor='lightgray', alpha=0.8))\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    # Additional insights\n",
        "    print(f\"\\nüîç Key Insights:\")\n",
        "    print(f\"   ‚Ä¢ Average chunk size: {word_count_stats['mean']:.1f} words\")\n",
        "    print(f\"   ‚Ä¢ Most chunks are between {word_count_stats['q25']:.0f} and {word_count_stats['q75']:.0f} words\")\n",
        "    print(f\"   ‚Ä¢ {sum(1 for wc in word_counts if wc < 50)} chunks ({sum(1 for wc in word_counts if wc < 50)/len(word_counts)*100:.1f}%) are very short (< 50 words)\")\n",
        "    print(f\"   ‚Ä¢ {sum(1 for wc in word_counts if wc > 500)} chunks ({sum(1 for wc in word_counts if wc > 500)/len(word_counts)*100:.1f}%) are very long (> 500 words)\")\n",
        "    \n",
        "else:\n",
        "    print(\"‚ùå No data available for analysis\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 2. Embedding Dimensionality and Properties Analysis\n",
        "\n",
        "if data and chunks:\n",
        "    # Extract embeddings\n",
        "    embeddings = []\n",
        "    embedding_lengths = []\n",
        "    \n",
        "    for chunk in chunks:\n",
        "        if 'embedding' in chunk and chunk['embedding']:\n",
        "            embedding = np.array(chunk['embedding'])\n",
        "            embeddings.append(embedding)\n",
        "            embedding_lengths.append(len(embedding))\n",
        "    \n",
        "    if embeddings:\n",
        "        embeddings_matrix = np.array(embeddings)\n",
        "        \n",
        "        print(f\"üî¢ Embedding Analysis:\")\n",
        "        print(f\"   ‚Ä¢ Total embeddings: {len(embeddings):,}\")\n",
        "        print(f\"   ‚Ä¢ Embedding dimension: {embeddings_matrix.shape[1]:,}\")\n",
        "        print(f\"   ‚Ä¢ Matrix shape: {embeddings_matrix.shape}\")\n",
        "        \n",
        "        # Calculate statistics\n",
        "        embedding_stats = {\n",
        "            'mean_norm': np.mean([np.linalg.norm(emb) for emb in embeddings]),\n",
        "            'std_norm': np.std([np.linalg.norm(emb) for emb in embeddings]),\n",
        "            'mean_values': np.mean(embeddings_matrix, axis=0),\n",
        "            'std_values': np.std(embeddings_matrix, axis=0),\n",
        "            'min_value': np.min(embeddings_matrix),\n",
        "            'max_value': np.max(embeddings_matrix),\n",
        "            'sparsity': np.mean(embeddings_matrix == 0) * 100\n",
        "        }\n",
        "        \n",
        "        # Create visualization\n",
        "        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))\n",
        "        fig.suptitle('üßÆ Embedding Properties Analysis', fontsize=16, fontweight='bold')\n",
        "        \n",
        "        # 1. Distribution of embedding norms\n",
        "        norms = [np.linalg.norm(emb) for emb in embeddings]\n",
        "        ax1.hist(norms, bins=50, alpha=0.7, color='coral', edgecolor='black')\n",
        "        ax1.axvline(embedding_stats['mean_norm'], color='red', linestyle='--', \n",
        "                   label=f\"Mean: {embedding_stats['mean_norm']:.3f}\")\n",
        "        ax1.set_xlabel('L2 Norm')\n",
        "        ax1.set_ylabel('Frequency')\n",
        "        ax1.set_title('Distribution of Embedding Norms')\n",
        "        ax1.legend()\n",
        "        ax1.grid(True, alpha=0.3)\n",
        "        \n",
        "        # 2. Distribution of embedding values\n",
        "        flat_embeddings = embeddings_matrix.flatten()\n",
        "        ax2.hist(flat_embeddings, bins=100, alpha=0.7, color='lightgreen', edgecolor='black')\n",
        "        ax2.set_xlabel('Embedding Value')\n",
        "        ax2.set_ylabel('Frequency')\n",
        "        ax2.set_title('Distribution of All Embedding Values')\n",
        "        ax2.grid(True, alpha=0.3)\n",
        "        \n",
        "        # 3. Mean and std per dimension (sample first 100 dimensions)\n",
        "        dims_to_show = min(100, len(embedding_stats['mean_values']))\n",
        "        x_dims = range(dims_to_show)\n",
        "        ax3.plot(x_dims, embedding_stats['mean_values'][:dims_to_show], \n",
        "                label='Mean', alpha=0.7, linewidth=2)\n",
        "        ax3.fill_between(x_dims, \n",
        "                        embedding_stats['mean_values'][:dims_to_show] - embedding_stats['std_values'][:dims_to_show],\n",
        "                        embedding_stats['mean_values'][:dims_to_show] + embedding_stats['std_values'][:dims_to_show],\n",
        "                        alpha=0.3, label='¬±1 Std Dev')\n",
        "        ax3.set_xlabel('Dimension')\n",
        "        ax3.set_ylabel('Value')\n",
        "        ax3.set_title(f'Mean ¬± Std Dev per Dimension (First {dims_to_show})')\n",
        "        ax3.legend()\n",
        "        ax3.grid(True, alpha=0.3)\n",
        "        \n",
        "        # 4. Statistics summary\n",
        "        ax4.axis('off')\n",
        "        stats_text = f\"\"\"\n",
        "        üßÆ Embedding Statistics\n",
        "        \n",
        "        Dimensions:     {embeddings_matrix.shape[1]:,}\n",
        "        Total vectors:  {len(embeddings):,}\n",
        "        \n",
        "        Norm Statistics:\n",
        "        Mean norm:      {embedding_stats['mean_norm']:.4f}\n",
        "        Std norm:       {embedding_stats['std_norm']:.4f}\n",
        "        \n",
        "        Value Range:\n",
        "        Min value:      {embedding_stats['min_value']:.4f}\n",
        "        Max value:      {embedding_stats['max_value']:.4f}\n",
        "        \n",
        "        Sparsity:       {embedding_stats['sparsity']:.2f}%\n",
        "        Memory usage:   {embeddings_matrix.nbytes / 1024 / 1024:.1f} MB\n",
        "        \"\"\"\n",
        "        ax4.text(0.1, 0.9, stats_text, transform=ax4.transAxes, fontsize=12,\n",
        "                 verticalalignment='top', fontfamily='monospace',\n",
        "                 bbox=dict(boxstyle='round', facecolor='lightgray', alpha=0.8))\n",
        "        \n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "        \n",
        "        # Additional analysis\n",
        "        print(f\"\\nüîç Embedding Quality Insights:\")\n",
        "        print(f\"   ‚Ä¢ Average embedding norm: {embedding_stats['mean_norm']:.4f}\")\n",
        "        print(f\"   ‚Ä¢ Value range: [{embedding_stats['min_value']:.4f}, {embedding_stats['max_value']:.4f}]\")\n",
        "        print(f\"   ‚Ä¢ Sparsity: {embedding_stats['sparsity']:.2f}% (zeros)\")\n",
        "        print(f\"   ‚Ä¢ Memory usage: {embeddings_matrix.nbytes / 1024 / 1024:.1f} MB\")\n",
        "        \n",
        "    else:\n",
        "        print(\"‚ùå No embeddings found in the data\")\n",
        "else:\n",
        "    print(\"‚ùå No data available for embedding analysis\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 3. Content Type and Document Structure Analysis\n",
        "\n",
        "if data and chunks:\n",
        "    # Extract content metadata\n",
        "    chunk_types = [chunk.get('chunk_type', 'unknown') for chunk in chunks]\n",
        "    doc_ids = [chunk.get('doc_id', 'unknown') for chunk in chunks]\n",
        "    section_paths = [chunk.get('section_path', []) for chunk in chunks]\n",
        "    \n",
        "    # Analyze section depths\n",
        "    section_depths = [len(path) if path else 0 for path in section_paths]\n",
        "    \n",
        "    # Count occurrences\n",
        "    type_counts = Counter(chunk_types)\n",
        "    doc_counts = Counter(doc_ids)\n",
        "    depth_counts = Counter(section_depths)\n",
        "    \n",
        "    # Create visualization\n",
        "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))\n",
        "    fig.suptitle('üìÑ Content Type and Structure Analysis', fontsize=16, fontweight='bold')\n",
        "    \n",
        "    # 1. Chunk types distribution\n",
        "    types, type_values = zip(*type_counts.most_common())\n",
        "    colors1 = plt.cm.Set3(np.linspace(0, 1, len(types)))\n",
        "    ax1.pie(type_values, labels=types, autopct='%1.1f%%', colors=colors1, startangle=90)\n",
        "    ax1.set_title('Distribution of Chunk Types')\n",
        "    \n",
        "    # 2. Document distribution\n",
        "    docs, doc_values = zip(*doc_counts.most_common())\n",
        "    ax2.bar(range(len(docs)), doc_values, color='lightcoral', alpha=0.7)\n",
        "    ax2.set_xlabel('Document')\n",
        "    ax2.set_ylabel('Number of Chunks')\n",
        "    ax2.set_title('Chunks per Document')\n",
        "    ax2.set_xticks(range(len(docs)))\n",
        "    ax2.set_xticklabels([doc[:20] + '...' if len(doc) > 20 else doc for doc in docs], \n",
        "                       rotation=45, ha='right')\n",
        "    ax2.grid(True, alpha=0.3)\n",
        "    \n",
        "    # 3. Section depth distribution\n",
        "    depths, depth_values = zip(*sorted(depth_counts.items()))\n",
        "    ax3.bar(depths, depth_values, color='lightblue', alpha=0.7, edgecolor='black')\n",
        "    ax3.set_xlabel('Section Depth')\n",
        "    ax3.set_ylabel('Number of Chunks')\n",
        "    ax3.set_title('Distribution of Section Depths')\n",
        "    ax3.grid(True, alpha=0.3)\n",
        "    \n",
        "    # 4. Summary statistics\n",
        "    ax4.axis('off')\n",
        "    \n",
        "    # Get top sections\n",
        "    all_sections = []\n",
        "    for path in section_paths:\n",
        "        if path:\n",
        "            all_sections.extend(path)\n",
        "    section_counter = Counter(all_sections)\n",
        "    top_sections = section_counter.most_common(5)\n",
        "    \n",
        "    summary_text = f\"\"\"\n",
        "    üìä Content Structure Summary\n",
        "    \n",
        "    Chunk Types:\n",
        "    {chr(10).join([f'    ‚Ä¢ {t}: {v:,} ({v/len(chunks)*100:.1f}%)' for t, v in type_counts.most_common()])}\n",
        "    \n",
        "    Documents:\n",
        "    {chr(10).join([f'    ‚Ä¢ {d[:30]}: {v:,}' for d, v in list(doc_counts.most_common())[:3]])}\n",
        "    \n",
        "    Section Depths:\n",
        "    ‚Ä¢ Average depth: {np.mean(section_depths):.1f}\n",
        "    ‚Ä¢ Max depth: {max(section_depths)}\n",
        "    ‚Ä¢ Chunks with sections: {sum(1 for d in section_depths if d > 0):,}\n",
        "    \n",
        "    Top Sections:\n",
        "    {chr(10).join([f'    ‚Ä¢ {s[:25]}: {c}' for s, c in top_sections[:3]])}\n",
        "    \"\"\"\n",
        "    \n",
        "    ax4.text(0.05, 0.95, summary_text, transform=ax4.transAxes, fontsize=10,\n",
        "             verticalalignment='top', fontfamily='monospace',\n",
        "             bbox=dict(boxstyle='round', facecolor='lightgray', alpha=0.8))\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    # Detailed insights\n",
        "    print(f\"\\nüîç Content Structure Insights:\")\n",
        "    print(f\"   ‚Ä¢ Total unique documents: {len(doc_counts)}\")\n",
        "    print(f\"   ‚Ä¢ Most common chunk type: {type_counts.most_common(1)[0][0]} ({type_counts.most_common(1)[0][1]:,} chunks)\")\n",
        "    print(f\"   ‚Ä¢ Average section depth: {np.mean(section_depths):.1f}\")\n",
        "    print(f\"   ‚Ä¢ {sum(1 for d in section_depths if d == 0):,} chunks ({sum(1 for d in section_depths if d == 0)/len(chunks)*100:.1f}%) have no section structure\")\n",
        "    \n",
        "else:\n",
        "    print(\"‚ùå No data available for content analysis\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 4. Correlation Analysis: Chunk Properties vs Embedding Characteristics\n",
        "\n",
        "if data and chunks and embeddings:\n",
        "    # Create DataFrame for correlation analysis\n",
        "    analysis_data = []\n",
        "    \n",
        "    for i, chunk in enumerate(chunks):\n",
        "        if i < len(embeddings):  # Ensure we have embedding for this chunk\n",
        "            embedding = embeddings[i]\n",
        "            \n",
        "            # Calculate embedding characteristics\n",
        "            emb_norm = np.linalg.norm(embedding)\n",
        "            emb_mean = np.mean(embedding)\n",
        "            emb_std = np.std(embedding)\n",
        "            emb_max = np.max(embedding)\n",
        "            emb_min = np.min(embedding)\n",
        "            emb_range = emb_max - emb_min\n",
        "            \n",
        "            # Get chunk properties\n",
        "            word_count = chunk.get('word_count', 0)\n",
        "            section_depth = len(chunk.get('section_path', []))\n",
        "            content_length = len(str(chunk.get('content', '')))\n",
        "            \n",
        "            analysis_data.append({\n",
        "                'word_count': word_count,\n",
        "                'content_length': content_length,\n",
        "                'section_depth': section_depth,\n",
        "                'embedding_norm': emb_norm,\n",
        "                'embedding_mean': emb_mean,\n",
        "                'embedding_std': emb_std,\n",
        "                'embedding_range': emb_range,\n",
        "                'embedding_max': emb_max,\n",
        "                'embedding_min': emb_min\n",
        "            })\n",
        "    \n",
        "    df = pd.DataFrame(analysis_data)\n",
        "    \n",
        "    # Calculate correlation matrix\n",
        "    correlation_matrix = df.corr()\n",
        "    \n",
        "    # Create visualization\n",
        "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))\n",
        "    fig.suptitle('üîó Correlation Analysis: Chunk Properties vs Embedding Characteristics', fontsize=16, fontweight='bold')\n",
        "    \n",
        "    # 1. Correlation heatmap\n",
        "    sns.heatmap(correlation_matrix, annot=True, cmap='RdBu_r', center=0,\n",
        "                square=True, ax=ax1, fmt='.3f', cbar_kws={'shrink': 0.8})\n",
        "    ax1.set_title('Correlation Matrix')\n",
        "    ax1.tick_params(axis='x', rotation=45)\n",
        "    ax1.tick_params(axis='y', rotation=0)\n",
        "    \n",
        "    # 2. Word count vs embedding norm scatter\n",
        "    ax2.scatter(df['word_count'], df['embedding_norm'], alpha=0.6, s=20, color='blue')\n",
        "    ax2.set_xlabel('Word Count')\n",
        "    ax2.set_ylabel('Embedding Norm')\n",
        "    ax2.set_title('Word Count vs Embedding Norm')\n",
        "    ax2.grid(True, alpha=0.3)\n",
        "    \n",
        "    # Add correlation coefficient\n",
        "    corr_coef = df['word_count'].corr(df['embedding_norm'])\n",
        "    ax2.text(0.05, 0.95, f'Correlation: {corr_coef:.3f}', transform=ax2.transAxes,\n",
        "             bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
        "    \n",
        "    # 3. Content length vs embedding std\n",
        "    ax3.scatter(df['content_length'], df['embedding_std'], alpha=0.6, s=20, color='green')\n",
        "    ax3.set_xlabel('Content Length (characters)')\n",
        "    ax3.set_ylabel('Embedding Std Dev')\n",
        "    ax3.set_title('Content Length vs Embedding Variability')\n",
        "    ax3.grid(True, alpha=0.3)\n",
        "    \n",
        "    corr_coef2 = df['content_length'].corr(df['embedding_std'])\n",
        "    ax3.text(0.05, 0.95, f'Correlation: {corr_coef2:.3f}', transform=ax3.transAxes,\n",
        "             bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
        "    \n",
        "    # 4. Section depth analysis\n",
        "    depth_stats = df.groupby('section_depth').agg({\n",
        "        'embedding_norm': ['mean', 'std'],\n",
        "        'word_count': 'count'\n",
        "    }).round(3)\n",
        "    \n",
        "    # Flatten column names\n",
        "    depth_stats.columns = ['_'.join(col).strip() for col in depth_stats.columns]\n",
        "    depth_stats = depth_stats.reset_index()\n",
        "    \n",
        "    if len(depth_stats) > 1:\n",
        "        ax4.bar(depth_stats['section_depth'], depth_stats['embedding_norm_mean'], \n",
        "                yerr=depth_stats['embedding_norm_std'], capsize=5, alpha=0.7, color='orange')\n",
        "        ax4.set_xlabel('Section Depth')\n",
        "        ax4.set_ylabel('Mean Embedding Norm')\n",
        "        ax4.set_title('Embedding Norm by Section Depth')\n",
        "        ax4.grid(True, alpha=0.3)\n",
        "    else:\n",
        "        ax4.text(0.5, 0.5, 'Insufficient section depth\\nvariability for analysis', \n",
        "                ha='center', va='center', transform=ax4.transAxes,\n",
        "                bbox=dict(boxstyle='round', facecolor='lightgray', alpha=0.8))\n",
        "        ax4.set_title('Section Depth Analysis')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    # Print key correlations\n",
        "    print(f\"\\nüîç Key Correlations:\")\n",
        "    print(f\"   ‚Ä¢ Word count ‚Üî Embedding norm: {df['word_count'].corr(df['embedding_norm']):.3f}\")\n",
        "    print(f\"   ‚Ä¢ Content length ‚Üî Embedding std: {df['content_length'].corr(df['embedding_std']):.3f}\")\n",
        "    print(f\"   ‚Ä¢ Section depth ‚Üî Embedding norm: {df['section_depth'].corr(df['embedding_norm']):.3f}\")\n",
        "    print(f\"   ‚Ä¢ Word count ‚Üî Content length: {df['word_count'].corr(df['content_length']):.3f}\")\n",
        "    \n",
        "    # Find strongest correlations with embedding properties\n",
        "    emb_cols = ['embedding_norm', 'embedding_mean', 'embedding_std', 'embedding_range']\n",
        "    content_cols = ['word_count', 'content_length', 'section_depth']\n",
        "    \n",
        "    print(f\"\\nüìä Strongest Correlations with Embedding Properties:\")\n",
        "    for emb_col in emb_cols:\n",
        "        for content_col in content_cols:\n",
        "            corr = df[content_col].corr(df[emb_col])\n",
        "            if abs(corr) > 0.1:  # Only show meaningful correlations\n",
        "                print(f\"   ‚Ä¢ {content_col} ‚Üî {emb_col}: {corr:.3f}\")\n",
        "    \n",
        "else:\n",
        "    print(\"‚ùå No data available for correlation analysis\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 5. Clustering Analysis: Discovering Natural Groupings in Embedding Space\n",
        "\n",
        "if data and chunks and embeddings and len(embeddings) > 10:\n",
        "    from sklearn.cluster import KMeans\n",
        "    from sklearn.decomposition import PCA\n",
        "    from sklearn.manifold import TSNE\n",
        "    from sklearn.metrics import silhouette_score\n",
        "    \n",
        "    print(\"üî¨ Performing clustering analysis...\")\n",
        "    \n",
        "    # Prepare embedding matrix\n",
        "    X = np.array(embeddings)\n",
        "    \n",
        "    # Determine optimal number of clusters using elbow method\n",
        "    max_clusters = min(10, len(embeddings) // 10)  # Reasonable upper bound\n",
        "    if max_clusters >= 2:\n",
        "        inertias = []\n",
        "        silhouette_scores = []\n",
        "        k_range = range(2, max_clusters + 1)\n",
        "        \n",
        "        for k in k_range:\n",
        "            kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
        "            kmeans.fit(X)\n",
        "            inertias.append(kmeans.inertia_)\n",
        "            silhouette_scores.append(silhouette_score(X, kmeans.labels_))\n",
        "        \n",
        "        # Choose optimal k (highest silhouette score)\n",
        "        optimal_k = k_range[np.argmax(silhouette_scores)]\n",
        "        \n",
        "        # Perform final clustering\n",
        "        kmeans = KMeans(n_clusters=optimal_k, random_state=42, n_init=10)\n",
        "        cluster_labels = kmeans.fit_predict(X)\n",
        "        \n",
        "        # Dimensionality reduction for visualization\n",
        "        print(\"üìâ Reducing dimensionality for visualization...\")\n",
        "        \n",
        "        # PCA for 2D visualization\n",
        "        pca = PCA(n_components=2, random_state=42)\n",
        "        X_pca = pca.fit_transform(X)\n",
        "        \n",
        "        # t-SNE for better cluster separation (if not too many points)\n",
        "        if len(embeddings) <= 1000:\n",
        "            tsne = TSNE(n_components=2, random_state=42, perplexity=min(30, len(embeddings)-1))\n",
        "            X_tsne = tsne.fit_transform(X)\n",
        "        else:\n",
        "            X_tsne = None\n",
        "            print(\"‚ö†Ô∏è Skipping t-SNE due to large dataset size\")\n",
        "        \n",
        "        # Create visualization\n",
        "        fig_size = (16, 12) if X_tsne is not None else (16, 8)\n",
        "        n_plots = 4 if X_tsne is not None else 3\n",
        "        fig, axes = plt.subplots(2, 2 if X_tsne is not None else 2, figsize=fig_size)\n",
        "        if X_tsne is None:\n",
        "            axes = axes.flatten()[:3]\n",
        "        else:\n",
        "            axes = axes.flatten()\n",
        "        \n",
        "        fig.suptitle('üéØ Clustering Analysis: Natural Groupings in Embedding Space', fontsize=16, fontweight='bold')\n",
        "        \n",
        "        # 1. Elbow method plot\n",
        "        axes[0].plot(k_range, inertias, 'bo-', linewidth=2, markersize=8)\n",
        "        axes[0].axvline(optimal_k, color='red', linestyle='--', label=f'Optimal k={optimal_k}')\n",
        "        axes[0].set_xlabel('Number of Clusters (k)')\n",
        "        axes[0].set_ylabel('Inertia')\n",
        "        axes[0].set_title('Elbow Method for Optimal k')\n",
        "        axes[0].legend()\n",
        "        axes[0].grid(True, alpha=0.3)\n",
        "        \n",
        "        # 2. Silhouette scores\n",
        "        axes[1].plot(k_range, silhouette_scores, 'ro-', linewidth=2, markersize=8)\n",
        "        axes[1].axvline(optimal_k, color='red', linestyle='--', label=f'Optimal k={optimal_k}')\n",
        "        axes[1].set_xlabel('Number of Clusters (k)')\n",
        "        axes[1].set_ylabel('Silhouette Score')\n",
        "        axes[1].set_title('Silhouette Analysis')\n",
        "        axes[1].legend()\n",
        "        axes[1].grid(True, alpha=0.3)\n",
        "        \n",
        "        # 3. PCA visualization\n",
        "        colors = plt.cm.tab10(np.linspace(0, 1, optimal_k))\n",
        "        for i in range(optimal_k):\n",
        "            mask = cluster_labels == i\n",
        "            axes[2].scatter(X_pca[mask, 0], X_pca[mask, 1], \n",
        "                           c=[colors[i]], label=f'Cluster {i}', alpha=0.7, s=30)\n",
        "        \n",
        "        axes[2].set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%} variance)')\n",
        "        axes[2].set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%} variance)')\n",
        "        axes[2].set_title('PCA Visualization of Clusters')\n",
        "        axes[2].legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "        axes[2].grid(True, alpha=0.3)\n",
        "        \n",
        "        # 4. t-SNE visualization (if available)\n",
        "        if X_tsne is not None:\n",
        "            for i in range(optimal_k):\n",
        "                mask = cluster_labels == i\n",
        "                axes[3].scatter(X_tsne[mask, 0], X_tsne[mask, 1], \n",
        "                               c=[colors[i]], label=f'Cluster {i}', alpha=0.7, s=30)\n",
        "            \n",
        "            axes[3].set_xlabel('t-SNE 1')\n",
        "            axes[3].set_ylabel('t-SNE 2')\n",
        "            axes[3].set_title('t-SNE Visualization of Clusters')\n",
        "            axes[3].legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "            axes[3].grid(True, alpha=0.3)\n",
        "        \n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "        \n",
        "        # Analyze cluster characteristics\n",
        "        print(f\"\\nüéØ Clustering Results:\")\n",
        "        print(f\"   ‚Ä¢ Optimal number of clusters: {optimal_k}\")\n",
        "        print(f\"   ‚Ä¢ Silhouette score: {silhouette_scores[optimal_k-2]:.3f}\")\n",
        "        print(f\"   ‚Ä¢ PCA explained variance: {sum(pca.explained_variance_ratio_):.1%}\")\n",
        "        \n",
        "        # Cluster size analysis\n",
        "        cluster_sizes = Counter(cluster_labels)\n",
        "        print(f\"\\nüìä Cluster Sizes:\")\n",
        "        for cluster_id in sorted(cluster_sizes.keys()):\n",
        "            size = cluster_sizes[cluster_id]\n",
        "            percentage = size / len(cluster_labels) * 100\n",
        "            print(f\"   ‚Ä¢ Cluster {cluster_id}: {size:,} chunks ({percentage:.1f}%)\")\n",
        "        \n",
        "        # Analyze cluster content characteristics\n",
        "        print(f\"\\nüîç Cluster Characteristics:\")\n",
        "        for cluster_id in range(optimal_k):\n",
        "            mask = cluster_labels == cluster_id\n",
        "            cluster_chunks = [chunks[i] for i in range(len(chunks)) if i < len(mask) and mask[i]]\n",
        "            \n",
        "            if cluster_chunks:\n",
        "                # Calculate statistics for this cluster\n",
        "                word_counts = [chunk.get('word_count', 0) for chunk in cluster_chunks]\n",
        "                doc_ids = [chunk.get('doc_id', 'unknown') for chunk in cluster_chunks]\n",
        "                chunk_types = [chunk.get('chunk_type', 'unknown') for chunk in cluster_chunks]\n",
        "                \n",
        "                avg_words = np.mean(word_counts) if word_counts else 0\n",
        "                most_common_doc = Counter(doc_ids).most_common(1)[0] if doc_ids else ('unknown', 0)\n",
        "                most_common_type = Counter(chunk_types).most_common(1)[0] if chunk_types else ('unknown', 0)\n",
        "                \n",
        "                print(f\"   Cluster {cluster_id}:\")\n",
        "                print(f\"     ‚Ä¢ Avg words: {avg_words:.1f}\")\n",
        "                print(f\"     ‚Ä¢ Top document: {most_common_doc[0][:30]} ({most_common_doc[1]} chunks)\")\n",
        "                print(f\"     ‚Ä¢ Top type: {most_common_type[0]} ({most_common_type[1]} chunks)\")\n",
        "        \n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è Not enough data points for meaningful clustering analysis\")\n",
        "        \n",
        "else:\n",
        "    print(\"‚ùå Insufficient data for clustering analysis\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 6. Summary Report and Recommendations\n",
        "\n",
        "print(\"üìã EMBEDDING ANALYSIS SUMMARY REPORT\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "if data and chunks:\n",
        "    # Overall statistics\n",
        "    total_chunks = len(chunks)\n",
        "    total_embeddings = len(embeddings) if 'embeddings' in locals() else 0\n",
        "    \n",
        "    print(f\"\\nüìä DATASET OVERVIEW:\")\n",
        "    print(f\"   ‚Ä¢ Total chunks processed: {total_chunks:,}\")\n",
        "    print(f\"   ‚Ä¢ Embeddings generated: {total_embeddings:,}\")\n",
        "    print(f\"   ‚Ä¢ Coverage: {total_embeddings/total_chunks*100:.1f}%\" if total_chunks > 0 else \"   ‚Ä¢ Coverage: 0%\")\n",
        "    \n",
        "    if 'word_count_stats' in locals():\n",
        "        print(f\"\\nüìè CHUNK SIZE ANALYSIS:\")\n",
        "        print(f\"   ‚Ä¢ Average chunk size: {word_count_stats['mean']:.1f} words\")\n",
        "        print(f\"   ‚Ä¢ Size range: {word_count_stats['min']}-{word_count_stats['max']} words\")\n",
        "        print(f\"   ‚Ä¢ Standard deviation: {word_count_stats['std']:.1f} words\")\n",
        "        \n",
        "        # Quality assessment\n",
        "        very_short = sum(1 for wc in word_counts if wc < 50)\n",
        "        very_long = sum(1 for wc in word_counts if wc > 500)\n",
        "        optimal_range = sum(1 for wc in word_counts if 100 <= wc <= 300)\n",
        "        \n",
        "        print(f\"   ‚Ä¢ Very short chunks (< 50 words): {very_short:,} ({very_short/total_chunks*100:.1f}%)\")\n",
        "        print(f\"   ‚Ä¢ Very long chunks (> 500 words): {very_long:,} ({very_long/total_chunks*100:.1f}%)\")\n",
        "        print(f\"   ‚Ä¢ Optimal range (100-300 words): {optimal_range:,} ({optimal_range/total_chunks*100:.1f}%)\")\n",
        "    \n",
        "    if 'embedding_stats' in locals():\n",
        "        print(f\"\\nüßÆ EMBEDDING QUALITY:\")\n",
        "        print(f\"   ‚Ä¢ Embedding dimension: {embeddings_matrix.shape[1]:,}\")\n",
        "        print(f\"   ‚Ä¢ Average norm: {embedding_stats['mean_norm']:.4f}\")\n",
        "        print(f\"   ‚Ä¢ Value range: [{embedding_stats['min_value']:.4f}, {embedding_stats['max_value']:.4f}]\")\n",
        "        print(f\"   ‚Ä¢ Sparsity: {embedding_stats['sparsity']:.2f}%\")\n",
        "        print(f\"   ‚Ä¢ Memory usage: {embeddings_matrix.nbytes / 1024 / 1024:.1f} MB\")\n",
        "    \n",
        "    if 'type_counts' in locals():\n",
        "        print(f\"\\nüìÑ CONTENT DISTRIBUTION:\")\n",
        "        for chunk_type, count in type_counts.most_common():\n",
        "            print(f\"   ‚Ä¢ {chunk_type}: {count:,} chunks ({count/total_chunks*100:.1f}%)\")\n",
        "    \n",
        "    if 'doc_counts' in locals():\n",
        "        print(f\"\\nüìö DOCUMENT DISTRIBUTION:\")\n",
        "        print(f\"   ‚Ä¢ Total documents: {len(doc_counts)}\")\n",
        "        for doc_id, count in list(doc_counts.most_common())[:5]:\n",
        "            print(f\"   ‚Ä¢ {doc_id[:40]}: {count:,} chunks\")\n",
        "        if len(doc_counts) > 5:\n",
        "            print(f\"   ‚Ä¢ ... and {len(doc_counts)-5} more documents\")\n",
        "    \n",
        "    if 'optimal_k' in locals():\n",
        "        print(f\"\\nüéØ CLUSTERING INSIGHTS:\")\n",
        "        print(f\"   ‚Ä¢ Natural clusters found: {optimal_k}\")\n",
        "        print(f\"   ‚Ä¢ Clustering quality (silhouette): {silhouette_scores[optimal_k-2]:.3f}\")\n",
        "        print(f\"   ‚Ä¢ PCA variance explained: {sum(pca.explained_variance_ratio_):.1%}\")\n",
        "    \n",
        "    # Recommendations\n",
        "    print(f\"\\nüí° RECOMMENDATIONS:\")\n",
        "    \n",
        "    if 'word_count_stats' in locals():\n",
        "        if word_count_stats['std'] > word_count_stats['mean'] * 0.5:\n",
        "            print(\"   ‚ö†Ô∏è  High variability in chunk sizes - consider more consistent chunking\")\n",
        "        \n",
        "        if very_short / total_chunks > 0.1:\n",
        "            print(\"   ‚ö†Ô∏è  Many very short chunks - may impact embedding quality\")\n",
        "        \n",
        "        if very_long / total_chunks > 0.05:\n",
        "            print(\"   ‚ö†Ô∏è  Some very long chunks - consider breaking them down further\")\n",
        "        \n",
        "        if optimal_range / total_chunks > 0.7:\n",
        "            print(\"   ‚úÖ Good chunk size distribution - most chunks in optimal range\")\n",
        "    \n",
        "    if 'embedding_stats' in locals():\n",
        "        if embedding_stats['sparsity'] > 10:\n",
        "            print(\"   ‚ö†Ô∏è  High sparsity in embeddings - check embedding model performance\")\n",
        "        \n",
        "        if embedding_stats['mean_norm'] < 0.1 or embedding_stats['mean_norm'] > 10:\n",
        "            print(\"   ‚ö†Ô∏è  Unusual embedding norms - verify embedding model configuration\")\n",
        "        else:\n",
        "            print(\"   ‚úÖ Embedding norms appear normal\")\n",
        "    \n",
        "    if 'optimal_k' in locals():\n",
        "        if silhouette_scores[optimal_k-2] > 0.5:\n",
        "            print(\"   ‚úÖ Strong natural clustering - good for semantic search\")\n",
        "        elif silhouette_scores[optimal_k-2] > 0.3:\n",
        "            print(\"   ‚úÖ Moderate clustering structure detected\")\n",
        "        else:\n",
        "            print(\"   ‚ö†Ô∏è  Weak clustering - embeddings may be too similar or noisy\")\n",
        "    \n",
        "    # Performance insights\n",
        "    if 'embeddings_matrix' in locals():\n",
        "        memory_mb = embeddings_matrix.nbytes / 1024 / 1024\n",
        "        if memory_mb > 1000:\n",
        "            print(\"   ‚ö†Ô∏è  Large memory footprint - consider dimensionality reduction\")\n",
        "        \n",
        "        search_complexity = total_embeddings * embeddings_matrix.shape[1]\n",
        "        if search_complexity > 1e8:\n",
        "            print(\"   ‚ö†Ô∏è  High search complexity - consider indexing strategies\")\n",
        "    \n",
        "    print(f\"\\nüéâ Analysis complete! Use these insights to optimize your RAG system.\")\n",
        "    \n",
        "else:\n",
        "    print(\"‚ùå No data available for summary report\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
